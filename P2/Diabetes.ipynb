{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9MwsPBe1n3c"
      },
      "source": [
        "# **Análisis libre**\n",
        "\n",
        "Realiza un análisis, el que el alumno/a desee, y considerando el conjunto de datos que quiera. Puede ser un análisis de Clustering, de cualquier tipo de algoritmo o algoritmos de clasificación, de reglas de asociación, etc.\n",
        "\n",
        "La calificación irá en función de:\n",
        "\n",
        "* Uso de técnicas no vistas en clase (ejemplos dados)\n",
        "* Explicación de los pasos seguidos\n",
        "* Interés del problema/solución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NsCQThF2r0f"
      },
      "source": [
        "# **Instalación del entorno**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhc57NWN51to"
      },
      "source": [
        "## Instalación de Hadoop\n",
        "\n",
        "Instalamos la versión de Hadoop/Spark 3.2.4\n",
        "Se recomienda visitar el sitio de Apache Spark para descargar esta versión:\n",
        "\n",
        "https://spark.apache.org/downloads.html\n",
        "\n",
        "Se configuran posteriormente las variables de entorno `JAVA_HOME` y `SPARK_HOME`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gknxsaLR2rUP"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9ThqOGJ2znW"
      },
      "source": [
        "La descarga de Hadoop puede tomar su tiempo, según la conexión disponible. Se borra posteriormente de la máquina virtual el archivo `.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAFtvd3M2y32"
      },
      "outputs": [],
      "source": [
        "!wget https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar -xf spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!rm spark-3.2.4-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouVI4Y1u2_52"
      },
      "source": [
        "## Instalación e iniciación de la sesión de Spark\n",
        "\n",
        "* Buscamos la librería `findspark` con `pip install`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk_qjQnV3C-l"
      },
      "outputs": [],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXzYYgtj3Fks"
      },
      "source": [
        "* Con `SparkSession` inicializamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNRJqXDr3LUM"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Spark_Dataframes\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2pGaFay4Gk4"
      },
      "source": [
        "# **Lectura del dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSSzNR2eG2cA"
      },
      "outputs": [],
      "source": [
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyyugbQ1G5SK"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "cdc_diabetes_health_indicators = fetch_ucirepo(id=891)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = cdc_diabetes_health_indicators.data.features\n",
        "y = cdc_diabetes_health_indicators.data.targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86ntqOGWJ8RE"
      },
      "source": [
        "# **Análisis Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTTPmgkAKAMf"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# (A) FETCH DATA & WRITE TO CSV (LOCAL)\n",
        "#######################################\n",
        "!pip install ucimlrepo pyspark cloudpickle seaborn scikit-learn --quiet\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1. Fetch data locally in pure Python/Pandas\n",
        "cdc_data = fetch_ucirepo(id=891)\n",
        "X = cdc_data.data.features\n",
        "y = cdc_data.data.targets\n",
        "data_df = pd.concat([X, y], axis=1)\n",
        "\n",
        "# 2. Convert to numeric in Pandas\n",
        "numeric_data_df = data_df.apply(pd.to_numeric, errors='coerce').fillna(0).astype(float)\n",
        "\n",
        "# 3. Save to CSV\n",
        "csv_path = \"/tmp/cdc_diabetes.csv\"\n",
        "numeric_data_df.to_csv(csv_path, index=False)\n",
        "\n",
        "########################################\n",
        "# (B) SPARK SESSION & KMEANS CLUSTERING\n",
        "########################################\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"HealthClustering\").getOrCreate()\n",
        "\n",
        "# 1. Read the CSV into Spark DataFrame\n",
        "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
        "\n",
        "# 2. Choose columns\n",
        "all_features = [\n",
        "    \"BMI\", \"MentHlth\", \"PhysHlth\", \"HighBP\", \"HighChol\",\n",
        "    \"Smoker\", \"PhysActivity\", \"Fruits\", \"Veggies\", \"HvyAlcoholConsump\",\n",
        "    \"Age\", \"Education\", \"Income\"\n",
        "]\n",
        "valid_features = [f for f in all_features if f in df.columns]\n",
        "data = df.select(valid_features)\n",
        "\n",
        "# 3. Fill missing with mean\n",
        "from pyspark.sql.functions import mean\n",
        "for f in valid_features:\n",
        "    mean_val = data.select(mean(f)).collect()[0][0]\n",
        "    if mean_val is not None:\n",
        "        data = data.fillna({f: mean_val})\n",
        "\n",
        "# 4. Assemble & scale\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "assembler = VectorAssembler(inputCols=valid_features, outputCol=\"features\")\n",
        "data = assembler.transform(data)\n",
        "\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "scaler_model = scaler.fit(data)\n",
        "data = scaler_model.transform(data)\n",
        "\n",
        "# 5. KMeans\n",
        "from pyspark.ml.clustering import KMeans\n",
        "kmeans = KMeans(featuresCol=\"scaledFeatures\", k=4, seed=1)\n",
        "model = kmeans.fit(data)\n",
        "predictions = model.transform(data)\n",
        "\n",
        "# 6. Silhouette score\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "evaluator = ClusteringEvaluator(featuresCol=\"scaledFeatures\", metricName=\"silhouette\")\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette Score:\", silhouette)\n",
        "\n",
        "# 7. Show cluster centers\n",
        "centers = model.clusterCenters()\n",
        "for i, c in enumerate(centers):\n",
        "    print(f\"Center {i}:\", c)\n",
        "\n",
        "# 8. Show cluster counts\n",
        "predictions.groupBy(\"prediction\").count().show()\n",
        "\n",
        "##########################################\n",
        "# (C) COLLECT TO PANDAS & STOP SPARK\n",
        "##########################################\n",
        "# 1. Convert Spark predictions to Pandas\n",
        "pdf = predictions.select(*valid_features, \"prediction\").toPandas()\n",
        "\n",
        "# 2. *Stop* Spark so we don't trigger pickling errors when plotting\n",
        "spark.stop()\n",
        "\n",
        "##########################################\n",
        "# (D) VISUALIZATION IN PURE PYTHON\n",
        "##########################################\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"whitegrid\")  # A nicer background\n",
        "\n",
        "###############\n",
        "# (D1) Bar Plot\n",
        "###############\n",
        "cluster_counts = pdf[\"prediction\"].value_counts().sort_index()\n",
        "df_counts = pd.DataFrame({\"cluster\": cluster_counts.index.astype(str),\n",
        "                          \"count\": cluster_counts.values})\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(data=df_counts, x=\"cluster\", y=\"count\", palette=\"Set2\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Number of Points per Cluster\")\n",
        "plt.show()\n",
        "\n",
        "################################\n",
        "# (D2) Optional PCA Scatter Plot\n",
        "################################\n",
        "# We'll project the scaled features to 2D with PCA\n",
        "# so we can visualize cluster separation.\n",
        "\n",
        "# We already used 'scaledFeatures' in Spark, but let's replicate that in Python:\n",
        "# - The easiest method is to re-assemble and scale with scikit-learn in Python,\n",
        "#   or re-collect \"scaledFeatures\" from Spark. For demonstration:\n",
        "#   We'll just use the raw columns from pdf and scale them with scikit-learn here.\n",
        "subpdf = pdf[valid_features].values  # raw features\n",
        "from sklearn.preprocessing import StandardScaler as SkScaler\n",
        "subpdf_scaled = SkScaler().fit_transform(subpdf)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_data = pca.fit_transform(subpdf_scaled)\n",
        "\n",
        "pdf[\"pca1\"] = pca_data[:, 0]\n",
        "pdf[\"pca2\"] = pca_data[:, 1]\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.scatterplot(\n",
        "    data=pdf, x=\"pca1\", y=\"pca2\", hue=\"prediction\",\n",
        "    palette=\"Set2\", alpha=0.6, edgecolor=None\n",
        ")\n",
        "plt.title(\"Clusters in 2D PCA space\")\n",
        "plt.legend(title=\"Cluster\", loc=\"best\")\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}